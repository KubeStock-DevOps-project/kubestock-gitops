# =============================================================================
# Prometheus Config - Production Environment
# =============================================================================
# Scrapes kubestock-production namespace services
# Includes alerting configuration
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'kubestock'
        environment: 'production'

    # Alertmanager configuration
    alerting:
      alertmanagers:
        - static_configs:
            - targets: ['alertmanager:9093']

    # Rule files
    rule_files:
      - '/etc/prometheus/alerts.yml'

    scrape_configs:
      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      # Alertmanager
      - job_name: 'alertmanager'
        static_configs:
          - targets: ['alertmanager:9093']

      # Kubernetes API server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Kubernetes nodes - via API server proxy (avoids kubelet TLS issues)
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

      # Kubernetes pods with prometheus.io annotations
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod

      # KubeStock Microservices - PRODUCTION namespace
      # Uses endpoints role to get pod IPs, filters out Istio sidecar ports
      - job_name: 'kubestock-services'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kubestock-production
        relabel_configs:
          # Keep only our microservices
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: ms-.+|web|frontend
          # Drop Istio sidecar ports (15xxx)
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: drop
            regex: .*-envoy-prom|tcp-.*
          - source_labels: [__meta_kubernetes_endpoint_port_number]
            action: drop
            regex: "15090|15020|15021"
          # Labels
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
        metrics_path: '/metrics'

      # Kong Gateway - Production
      - job_name: 'kong-gateway'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kong-production
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: kong.*
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service

      # kube-state-metrics - Production
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.observability-production.svc:8080']

      # Node Exporter - Production (port 9100)
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - observability-production
        relabel_configs:
          - source_labels: [__meta_kubernetes_endpoints_name]
            action: keep
            regex: node-exporter
          - source_labels: [__meta_kubernetes_endpoint_node_name]
            action: replace
            target_label: instance

      # cAdvisor metrics from kubelet
      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [__meta_kubernetes_node_name]
            target_label: node

  alerts.yml: |
    groups:
      - name: kubestock-alerts
        rules:
          # Service metrics endpoint down - PRIMARY ALERT for demo
          - alert: ServiceDown
            expr: up{job="kubestock-services"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service {{ $labels.service }} is down"
              description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has been down for more than 1 minute. Pod: {{ $labels.pod }}"
          
          # Deployment has no ready replicas
          - alert: DeploymentDown
            expr: kube_deployment_status_replicas_available{namespace="kubestock-production"} == 0 and kube_deployment_spec_replicas{namespace="kubestock-production"} > 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Deployment {{ $labels.deployment }} has no replicas"
              description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has no available replicas."

          - alert: HighErrorRate
            expr: |
              sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
              /
              sum(rate(http_requests_total[5m])) by (service) > 0.05
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High error rate on {{ $labels.service }}"
              description: "Service {{ $labels.service }} has error rate > 5% (current: {{ $value | humanizePercentage }})."

          # Pod Health
          - alert: PodRestarting
            expr: increase(kube_pod_container_status_restarts_total{namespace="kubestock-production"}[1h]) > 3
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} restarting frequently"
              description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour."

          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total{namespace="kubestock-production"}[15m]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Pod {{ $labels.pod }} is crash looping"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is in CrashLoopBackOff state."

          - alert: PodNotReady
            expr: kube_pod_status_phase{namespace="kubestock-production", phase!~"Running|Succeeded"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} not ready"
              description: "Pod {{ $labels.pod }} has been in {{ $labels.phase }} state for more than 5 minutes."

          # Resource Usage - Pods
          - alert: HighMemoryUsage
            expr: |
              (container_memory_working_set_bytes{namespace="kubestock-production", container!=""}
              / container_spec_memory_limit_bytes{namespace="kubestock-production", container!=""}) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit."

          - alert: HighCPUUsage
            expr: |
              (rate(container_cpu_usage_seconds_total{namespace="kubestock-production", container!=""}[5m])
              / container_spec_cpu_quota{namespace="kubestock-production", container!=""} * 100000) > 0.8
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of CPU limit."

          # Node Health
          - alert: NodeNotReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.node }} is not ready"
              description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes."

          - alert: NodeMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.node }} has memory pressure"
              description: "Node {{ $labels.node }} is experiencing memory pressure."

          - alert: NodeDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.node }} has disk pressure"
              description: "Node {{ $labels.node }} is experiencing disk pressure."

          # Storage
          - alert: PersistentVolumeNearFull
            expr: |
              (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"
              description: "PersistentVolumeClaim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is more than 80% full."

          - alert: PersistentVolumeCriticallyFull
            expr: |
              (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.9
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"
              description: "PersistentVolumeClaim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is critically full (>90%)."

          # Kubernetes Components
          - alert: KubernetesAPIServerDown
            expr: up{job="kubernetes-apiservers"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Kubernetes API server is down"
              description: "Kubernetes API server has been unreachable for more than 1 minute."

          # Database (PostgreSQL - if exposed metrics)
          - alert: DatabaseDown
            expr: up{job=~".*postgres.*"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Database {{ $labels.instance }} is down"
              description: "PostgreSQL database has been down for more than 1 minute."
